{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, ResNet50\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "from sklearn.externals import joblib \n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from functools import partial\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def w_categorical_crossentropy(y_true, y_pred, weights):\n",
    "    '''medida de loss com CatEnt, a partir de uma matriz de pesos, \"gabarito\" e previsoes '''\n",
    "    nb_cl = len(weights)\n",
    "    final_mask = K.zeros_like(y_pred[:, 0])\n",
    "    y_pred_max = K.max(y_pred, axis=1)\n",
    "    y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "    y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())\n",
    "    for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n",
    "    return K.categorical_crossentropy(y_pred, y_true) * final_mask\n",
    "\n",
    "with open('/home/julia/Hack/hackaton-cotidiano/notebooks/loss_matrix.p', \"rb\") as f: \n",
    "    #carregar matriz preparada para o problema dos vinhos\n",
    "    w_array = pickle.load(f)\n",
    "\n",
    "w_array = w_array.astype(np.float32)\n",
    "\n",
    "weighted_cat_crossentropy = partial(w_categorical_crossentropy, weights=w_array)\n",
    "weighted_cat_crossentropy.__name__ ='weighted_cat_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss = 'categorical_crossentropy'\n",
    "loss = weighted_cat_crossentropy\n",
    "print(\"Begin\")\n",
    "\n",
    "def create_symlinks():\n",
    "    ''' organizacao de pastas para o formato do keras '''\n",
    "    path = \"/media/5TB/Hackathon/data/food-101/images\"\n",
    "    df = pd.DataFrame(glob(path+\"/**/*.jpg\"), columns = [\"filename\"])\n",
    "    df[\"label\"] = df.filename.apply(lambda x: os.path.basename(os.path.dirname(x)))\n",
    "    print(np.unique(df.label, return_counts=True))\n",
    "    df = shuffle(df)\n",
    "    df = df.reset_index(drop=True)\n",
    "    train_ids = []\n",
    "    val_ids = []\n",
    "    test_ids = []\n",
    "    train_split= int(0.75*1000)\n",
    "    val_split = int(0.15*1000)\n",
    "\n",
    "    for label in np.unique(df.label):\n",
    "        current_df = df[df.label == label]\n",
    "        train_ids.extend(current_df.index[0:train_split])\n",
    "        val_ids.extend(current_df.index[train_split:train_split+val_split])\n",
    "        test_ids.extend(current_df.index[train_split+val_split:])\n",
    "        #for index, row in df[df.label == label].iterrows():\n",
    "        #    print(df[df.label == label].shape)\n",
    "    \n",
    "    print(len(train_ids), len(val_ids), len(test_ids))\n",
    "    assert len(train_ids) + len(val_ids) + len(test_ids) == len(df)\n",
    "    \n",
    "    basepath = \"/media/5TB/Hackathon/data/food-101-keras\"\n",
    "    os.makedirs(basepath, exist_ok=True)\n",
    "\n",
    "    for ids_list, split_name in zip([train_ids, val_ids, test_ids], [\"train\", \"val\", \"test\"]):\n",
    "        print(split_name, len(ids_list))\n",
    "        current_df = df[df.index.isin(ids_list)]\n",
    "\n",
    "        for index, row in tqdm(current_df.iterrows(), total=current_df.shape[0]):\n",
    "            dst_link = os.path.join(basepath, split_name, os.path.basename(os.path.dirname(row.filename)), os.path.basename(row.filename))\n",
    "            os.makedirs(os.path.dirname(dst_link), exist_ok=True)\n",
    "            #print(dst_link)\n",
    "            os.symlink(row.filename, dst_link)\n",
    "\n",
    "#np.unique(df[df.index.isin(train_ids)].label, return_counts=True)\n",
    "\n",
    "np.random.seed(20) #seed para reproducao\n",
    "\n",
    "\n",
    "#optimizer\n",
    "optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "#modelo pre treinado para transfer learning\n",
    "base_model = ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None)\n",
    "\n",
    "#arquitetura do final\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "outputs = Dense(101, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "for layer in base_model.layers[:-8]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "model_filename = \"weights/resnet_food_wloss_v0.3.h5\"\n",
    "\n",
    "#model_filename = \"weights/resnet_food_v0.2.h5\"\n",
    "model.load_weights(model_filename)\n",
    "model_filename = \"weights/resnet_food_wloss_v0.4.h5\"\n",
    "\n",
    "#model_filename = \"~/weights/resnet_food.h5\"\n",
    "#model.summary()\n",
    "print(\"Done loading\")\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "target_size = (224, 224)\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                  width_shift_range=0.2,\n",
    "    # augmentation para criar invariancia com escala, rotacao, ruidos diversos \n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    channel_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_path = \"/home/julia/Hack/resized_data_splits/train\"\n",
    "val_path= \"/home/julia/Hack/resized_data_splits/val\"\n",
    "test_path = \"/home/julia/Hack/resized_data_splits/test\"\n",
    "\n",
    "\n",
    "#flow de treino/teste\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=64,\n",
    "        class_mode='categorical', follow_links=True)\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=64,\n",
    "        class_mode='categorical', follow_links=True)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(model_filename, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "history = model.fit_generator( #checkpoint salvando apenas para melhoras\n",
    "        train_generator,\n",
    "        steps_per_epoch=150,\n",
    "        epochs=1000,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=10, callbacks=[model_checkpoint])\n",
    "\n",
    "model_final_filename = model_filename + \"_final\"\n",
    "model.save(model_final_filename)\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#class_dict = {v: k for k, v in train_generator.class_indices.items()}\n",
    "#joblib.dump(class_dict, \"class_dict.pkl\")\n",
    "'''\n",
    "def get_label(output):\n",
    "    output = np.argmax(output)\n",
    "    return class_dict[output]\n",
    "'''\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "'''\n",
    "img_path = '/media/julia/5TB/Hackathon/data/food-101-keras/train/apple_pie/1818676.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "out = model.predict(x)\n",
    "label = get_label(out)\n",
    "print(label)\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
